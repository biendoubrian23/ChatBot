"""Service intelligent pour analyser les messages utilisateurs avec LLM-first."""
import re
import json
import hashlib
import threading
from typing import Optional, Dict, Any
from app.services.llm import OllamaService


# Cache global thread-safe pour les intentions (cl√© = hash du message normalis√©)
_INTENT_CACHE: Dict[str, Dict[str, Any]] = {}
_CACHE_MAX_SIZE = 500  # Limiter la taille du cache
_CACHE_LOCK = threading.Lock()  # Verrou pour acc√®s concurrent


class MessageAnalyzer:
    """
    Analyse les messages avec le LLM comme cerveau principal.
    
    Flux LLM-First + Cache (Thread-Safe):
    1. V√©rifier si l'intention est en cache (avec verrou)
    2. Sinon, le LLM analyse le message et d√©termine l'intention
    3. Mettre en cache le r√©sultat (avec verrou)
    
    Optimis√© pour plusieurs utilisateurs simultan√©s.
    """
    
    def __init__(self, llm_service: OllamaService):
        """
        Initialize le MessageAnalyzer avec un service LLM.
        
        Args:
            llm_service: Instance du service Ollama pour l'analyse LLM
        """
        self.llm = llm_service
    
    def _get_cache_key(self, message: str) -> str:
        """G√©n√®re une cl√© de cache bas√©e sur le message normalis√©."""
        # Normaliser: minuscule, sans espaces multiples, sans ponctuation superflue
        normalized = re.sub(r'\s+', ' ', message.lower().strip())
        # Garder les chiffres intacts pour les num√©ros de commande
        return hashlib.md5(normalized.encode()).hexdigest()
    
    def _check_cache(self, message: str) -> Optional[Dict[str, Any]]:
        """V√©rifie si l'intention est en cache (thread-safe)."""
        key = self._get_cache_key(message)
        with _CACHE_LOCK:
            if key in _INTENT_CACHE:
                cached = _INTENT_CACHE[key].copy()
                cached["source"] = "cache"
                print(f"‚ö° Cache HIT: {cached['intent']}")
                return cached
        return None
    
    def _add_to_cache(self, message: str, analysis: Dict[str, Any]) -> None:
        """Ajoute une analyse au cache (thread-safe)."""
        global _INTENT_CACHE
        key = self._get_cache_key(message)
        
        with _CACHE_LOCK:
            # Limiter la taille du cache
            if len(_INTENT_CACHE) >= _CACHE_MAX_SIZE:
                # Supprimer les 100 plus anciennes entr√©es
                keys_to_remove = list(_INTENT_CACHE.keys())[:100]
                for k in keys_to_remove:
                    del _INTENT_CACHE[k]
            
            _INTENT_CACHE[key] = analysis.copy()
    
    async def analyze_with_llm(self, message: str) -> Dict[str, Any]:
        """
        Analyse compl√®te du message par le LLM.
        PROMPT OPTIMIS√â pour vitesse + qualit√©.
        
        Returns:
            {
                "intent": "order_tracking" | "general_question",
                "order_number": str | None,
                "reasoning": str
            }
        """
        # PROMPT OPTIMIS√â: distingue bien questions g√©n√©rales vs suivi de commande
        prompt = f"""Analyse ce message client CoolLibri (imprimerie livres):
"{message}"

INTENTION:
- ORDER_TRACKING = veut le STATUT/SUIVI de SA commande PERSONNELLE ("o√π en est MA commande?", "commande 13349", "mon colis?", "je veux suivre ma commande", juste un num√©ro de commande)
- GENERAL_QUESTION = questions G√âN√âRALES sur CoolLibri: d√©lais de livraison en g√©n√©ral, prix, formats, fonctionnement, annulation, r√©clamation, qualit√©, probl√®mes, remboursement

‚ö†Ô∏è ATTENTION: Si le client pose une question G√âN√âRALE sur les d√©lais ("quels sont les d√©lais de livraison?", "combien de temps pour recevoir un livre?", "d√©lais d'exp√©dition?") SANS parler de SA commande ‚Üí c'est GENERAL_QUESTION

NUM√âRO: Extrais UNIQUEMENT un num√©ro PR√âSENT dans le message. Sinon null.

JSON uniquement:
{{"intent":"ORDER_TRACKING|GENERAL_QUESTION","order_number":"xxxxx|null","reasoning":"court"}}"""

        try:
            # max_tokens r√©duit de 20%: 150 ‚Üí 120
            response = await self.llm.generate(prompt, max_tokens=120)
            response_clean = response.strip()
            
            # Nettoyer la r√©ponse pour extraire le JSON
            # Parfois le LLM ajoute des backticks ou du texte autour
            if "```json" in response_clean:
                response_clean = response_clean.split("```json")[1].split("```")[0]
            elif "```" in response_clean:
                response_clean = response_clean.split("```")[1].split("```")[0]
            
            # Trouver le JSON dans la r√©ponse
            json_match = re.search(r'\{[^{}]*\}', response_clean, re.DOTALL)
            if json_match:
                response_clean = json_match.group(0)
            
            # Parser le JSON
            result = json.loads(response_clean)
            
            intent = result.get("intent", "GENERAL_QUESTION").upper()
            order_number = result.get("order_number")
            reasoning = result.get("reasoning", "")
            
            # Normaliser l'intention
            if "ORDER" in intent or "TRACKING" in intent:
                intent = "order_tracking"
            else:
                intent = "general_question"
            
            # Nettoyer le num√©ro de commande
            if order_number and order_number != "null" and order_number != "None":
                # Extraire uniquement les chiffres
                order_number = re.sub(r'[^\d]', '', str(order_number))
                if not order_number or len(order_number) < 4:
                    order_number = None
                else:
                    # VALIDATION CRUCIALE: V√©rifier que le num√©ro existe VRAIMENT dans le message
                    if order_number not in message:
                        print(f"‚ö†Ô∏è LLM a invent√© un num√©ro ({order_number}) - ignor√© car absent du message")
                        order_number = None
            else:
                order_number = None
            
            print(f"üß† LLM Analysis: intent={intent}, order_number={order_number}, reasoning={reasoning}")
            
            return {
                "intent": intent,
                "order_number": order_number,
                "reasoning": reasoning,
                "source": "llm"
            }
            
        except json.JSONDecodeError as e:
            print(f"‚ö†Ô∏è Erreur parsing JSON LLM: {e}")
            print(f"   R√©ponse brute: {response_clean[:200] if 'response_clean' in dir() else 'N/A'}")
            # Fallback: essayer de d√©tecter l'intention dans la r√©ponse brute
            return self._fallback_analysis(message, response_clean if 'response_clean' in dir() else "")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur LLM: {e}")
            return self._fallback_analysis(message, "")
    
    def _fallback_analysis(self, message: str, llm_response: str) -> Dict[str, Any]:
        """
        Analyse de secours si le LLM √©choue ou retourne un JSON invalide.
        Utilise des heuristiques simples.
        """
        message_lower = message.lower()
        llm_response_upper = llm_response.upper()
        
        # Essayer de comprendre ce que le LLM voulait dire
        if "ORDER_TRACKING" in llm_response_upper or "ORDER" in llm_response_upper:
            intent = "order_tracking"
        elif "GENERAL" in llm_response_upper:
            intent = "general_question"
        else:
            # Heuristiques bas√©es sur le message original
            tracking_keywords = ["o√π en est", "suivi", "suivre", "tracker", "statut de ma commande"]
            general_keywords = ["annuler", "r√©clamation", "d√©faut", "floue", "qualit√©", "probl√®me", "remboursement", "rendu", "3d", "fichier"]
            
            has_tracking = any(kw in message_lower for kw in tracking_keywords)
            has_general = any(kw in message_lower for kw in general_keywords)
            
            if has_general:
                intent = "general_question"
            elif has_tracking:
                intent = "order_tracking"
            else:
                intent = "general_question"  # Par d√©faut
        
        # Essayer d'extraire un num√©ro de commande avec regex
        order_number = self._extract_order_number_regex(message)
        
        return {
            "intent": intent,
            "order_number": order_number,
            "reasoning": "Fallback analysis",
            "source": "fallback"
        }
    
    def _extract_order_number_regex(self, message: str) -> Optional[str]:
        """
        Extraction de num√©ro de commande par regex (utilis√© en fallback uniquement).
        """
        cleaned = message.lower().strip()
        
        patterns = [
            r'(?:commande|commandes|num√©ro|numero|n¬∞|#)\s*[:\s]*(\d{4,6})',
            r'(?:^|\s)(\d{5})(?:\s|$)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, cleaned)
            if match:
                return match.group(1)
        
        return None
    
    async def analyze_message(self, message: str) -> Dict[str, Any]:
        """
        Analyse compl√®te du message utilisateur.
        
        FLUX OPTIMIS√â:
        1. V√©rifier le cache
        2. Si pas en cache, le LLM analyse
        3. Mettre en cache le r√©sultat
        
        Returns:
            {
                "intent": "order_tracking" | "general_question",
                "order_number": str | None,
                "needs_order_input": bool,
                "confidence": "high" | "medium" | "low",
                "source": "llm" | "fallback" | "cache"
            }
        """
        # √âtape 1: V√©rifier le cache (TTFB ~0ms si hit)
        cached = self._check_cache(message)
        if cached:
            intent = cached["intent"]
            order_number = cached.get("order_number")
            # Re-extraire le num√©ro au cas o√π le message en contient un nouveau
            if not order_number:
                order_number = self._extract_order_number_regex(message)
            needs_order_input = (intent == "order_tracking" and order_number is None)
            return {
                "intent": intent,
                "order_number": order_number,
                "needs_order_input": needs_order_input,
                "confidence": "high",
                "source": "cache"
            }
        
        # √âtape 2: Le LLM analyse tout
        analysis = await self.analyze_with_llm(message)
        
        intent = analysis["intent"]
        order_number = analysis.get("order_number")
        source = analysis.get("source", "llm")
        
        # Mettre en cache (seulement si LLM a r√©pondu)
        if source == "llm":
            self._add_to_cache(message, analysis)
        
        # D√©terminer la confiance
        confidence = "high" if source == "llm" else "medium"
        
        # D√©terminer si on a besoin de demander le num√©ro
        needs_order_input = (intent == "order_tracking" and order_number is None)
        
        result = {
            "intent": intent,
            "order_number": order_number,
            "needs_order_input": needs_order_input,
            "confidence": confidence,
            "source": source
        }
        
        print(f"üìä Final Analysis: {result}")
        
        return result
