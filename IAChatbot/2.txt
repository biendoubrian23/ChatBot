# ğŸ¯ PLAN D'ACTION - Chatbot Intelligent CoolLibri
# ================================================
# Objectif: Atteindre 80-90% de la qualitÃ© ChatGPT/Claude
# Date: 29 DÃ©cembre 2025

## ğŸ“Š Ã‰TAT ACTUEL vs CIBLE

| FonctionnalitÃ©              | Actuel | Cible | PrioritÃ© |
|-----------------------------|--------|-------|----------|
| RÃ©ponses basiques           | âœ… 80% | 95%   | -        |
| Clarification questions     | âŒ 0%  | 90%   | ğŸ”´ HAUTE |
| Topic Tracking              | âŒ 0%  | 85%   | ğŸ”´ HAUTE |
| Contexte intelligent        | âš ï¸ 30% | 90%   | ğŸ”´ HAUTE |
| Score de confiance          | âŒ 0%  | 90%   | ğŸŸ¡ MOYENNE |
| MÃ©moire structurÃ©e          | âŒ 0%  | 80%   | ğŸŸ¡ MOYENNE |

---

## ğŸ—ï¸ ARCHITECTURE CIBLE

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        MESSAGE UTILISATEUR                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  NOUVEAU: ConversationManager (orchestrateur principal)             â”‚
â”‚  - GÃ¨re le contexte de conversation                                 â”‚
â”‚  - Coordonne les autres services                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ NOUVEAU:         â”‚  â”‚ AMÃ‰LIORÃ‰:        â”‚  â”‚ NOUVEAU:         â”‚
â”‚ TopicDetector    â”‚  â”‚ MessageAnalyzer  â”‚  â”‚ ContextManager   â”‚
â”‚                  â”‚  â”‚ + Confidence     â”‚  â”‚                  â”‚
â”‚ - DÃ©tecte sujet  â”‚  â”‚ - Score 0-100%   â”‚  â”‚ - MÃ©moire struct â”‚
â”‚ - Changement?    â”‚  â”‚ - AmbiguÃ¯tÃ©?     â”‚  â”‚ - Topics passÃ©s  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                       â”‚                       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Confiance < 70% ?    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚           â”‚
                    OUI  â”‚           â”‚  NON
                         â–¼           â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ NOUVEAU:         â”‚  â”‚ EXISTANT + AMÃ‰LIORÃ‰: â”‚
            â”‚ Clarification    â”‚  â”‚ RAG Pipeline         â”‚
            â”‚ Generator        â”‚  â”‚ + Prompt amÃ©liorÃ©    â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ… TODO LIST COMPLÃˆTE

### PHASE 1: FONDATIONS (PrioritÃ© ğŸ”´ HAUTE)
#### DurÃ©e estimÃ©e: 2-3 heures

- [ ] **1.1 CrÃ©er le service TopicDetector**
  - Fichier: `backend/app/services/topic_detector.py`
  - Fonctions:
    - `detect_topic(message)` â†’ retourne le sujet principal
    - `is_topic_change(current_topic, new_message)` â†’ bool
    - `get_topic_category(message)` â†’ dÃ©lais/formats/reliures/commande/prix/problÃ¨me/autre
  - Compatible: Ollama + Mistral + Groq (utilise le provider actif)

- [ ] **1.2 CrÃ©er le service ContextManager**
  - Fichier: `backend/app/services/context_manager.py`
  - Structure de donnÃ©es:
    ```python
    class ConversationContext:
        session_id: str
        current_topic: str
        previous_topics: List[TopicSummary]
        user_profile: UserProfile
        conversation_state: str  # greeting/info_gathering/problem_solving/closing
        messages_count: int
        created_at: datetime
        last_activity: datetime
    ```
  - Fonctions:
    - `get_context(session_id)` â†’ ConversationContext
    - `update_context(session_id, message, response, topic)` â†’ None
    - `should_use_context(current_topic, context)` â†’ bool
    - `get_relevant_context_summary(context, current_topic)` â†’ str
  - Stockage: En mÃ©moire avec TTL (1 heure)

- [ ] **1.3 AmÃ©liorer MessageAnalyzer avec Confidence Score**
  - Fichier: `backend/app/services/message_analyzer.py` (modifier)
  - Ajouter:
    - `confidence_score: int` (0-100) dans le retour
    - `ambiguity_type: str` (subject/intent/context/none)
    - `suggested_clarifications: List[str]`
  - Nouveau prompt LLM pour Ã©valuer l'ambiguÃ¯tÃ©

---

### PHASE 2: CLARIFICATION INTELLIGENTE (PrioritÃ© ğŸ”´ HAUTE)
#### DurÃ©e estimÃ©e: 1-2 heures

- [ ] **2.1 CrÃ©er le service ClarificationGenerator**
  - Fichier: `backend/app/services/clarification_generator.py`
  - Fonctions:
    - `needs_clarification(confidence_score, ambiguity_type)` â†’ bool
    - `generate_clarification(message, ambiguity_type, context)` â†’ str
  - Templates de clarification par type:
    ```python
    CLARIFICATION_TEMPLATES = {
        "subject": "Pourriez-vous prÃ©ciser votre demande ? Vous souhaitez des informations sur :\n- {options}",
        "intent": "Que souhaitez-vous faire exactement ?\n- {options}",
        "context": "Pour mieux vous aider, pouvez-vous prÃ©ciser :\n- {question}",
    }
    ```
  - Options dynamiques basÃ©es sur les sujets CoolLibri

- [ ] **2.2 DÃ©finir les catÃ©gories de clarification**
  - Fichier: `backend/app/services/clarification_generator.py`
  - CatÃ©gories:
    ```python
    CLARIFICATION_OPTIONS = {
        "modification": ["L'adresse de livraison", "Le format du livre", "La quantitÃ©", "Autre"],
        "problÃ¨me": ["QualitÃ© d'impression", "Colis abÃ®mÃ©", "Retard livraison", "Autre"],
        "information": ["Les formats disponibles", "Les dÃ©lais", "Les prix", "Les reliures"],
        "commande": ["Suivre ma commande", "Modifier ma commande", "Annuler ma commande"],
    }
    ```

---

### PHASE 3: PROMPT SYSTÃˆME AMÃ‰LIORÃ‰ (PrioritÃ© ğŸ”´ HAUTE)
#### DurÃ©e estimÃ©e: 1 heure

- [ ] **3.1 CrÃ©er un nouveau systÃ¨me de prompts**
  - Fichier: `backend/app/services/prompt_builder.py`
  - Fonctions:
    - `build_system_prompt(context: ConversationContext)` â†’ str
    - `build_user_prompt(query, rag_context, conversation_context)` â†’ str
  - Template dynamique qui injecte le contexte

- [ ] **3.2 Nouveau prompt systÃ¨me intelligent**
  ```python
  INTELLIGENT_SYSTEM_PROMPT = """
  Tu es l'assistant CoolLibri, service d'impression de livres.

  CONTEXTE CONVERSATION:
  - Sujet actuel: {current_topic}
  - Sujets prÃ©cÃ©dents: {previous_topics_summary}
  - Ã‰tat: {conversation_state}

  RÃˆGLES CRITIQUES:

  1. CLARIFICATION (si confiance < 70%):
     - Pose UNE question prÃ©cise pour comprendre
     - Ne devine JAMAIS si tu n'es pas sÃ»r

  2. CHANGEMENT DE SUJET:
     - Si l'utilisateur change de sujet, rÃ©ponds au NOUVEAU sujet uniquement
     - Ne force JAMAIS un lien avec le sujet prÃ©cÃ©dent
     - Exemple: "dÃ©lais?" puis "reliures?" â†’ parle des reliures, pas des dÃ©lais

  3. UTILISATION DU CONTEXTE:
     - Utilise le contexte SEULEMENT si pertinent naturellement
     - Si nouveau sujet indÃ©pendant â†’ ignore l'historique

  4. STYLE:
     - RÃ©ponses concises (4-6 phrases max)
     - Informations prÃ©cises (chiffres, options, prix)
     - Ton professionnel et chaleureux
  """
  ```

- [ ] **3.3 Mettre Ã  jour llm.py pour utiliser le nouveau prompt builder**
  - Modifier `_get_system_prompt()` pour accepter le contexte
  - Modifier `generate_response_stream_async()` pour passer le contexte

---

### PHASE 4: ORCHESTRATION (PrioritÃ© ğŸŸ¡ MOYENNE)
#### DurÃ©e estimÃ©e: 2 heures

- [ ] **4.1 CrÃ©er le ConversationManager (orchestrateur)**
  - Fichier: `backend/app/services/conversation_manager.py`
  - RÃ´le: Coordonne tous les services
  - Fonctions:
    - `process_message(session_id, message, history)` â†’ Response
    - Flux:
      1. RÃ©cupÃ©rer/crÃ©er contexte
      2. Analyser message (intention + confiance)
      3. DÃ©tecter le topic
      4. DÃ©cider: clarifier ou rÃ©pondre?
      5. Si clarifier â†’ gÃ©nÃ©rer question
      6. Si rÃ©pondre â†’ RAG + LLM avec contexte intelligent
      7. Mettre Ã  jour le contexte
      8. Retourner la rÃ©ponse

- [ ] **4.2 IntÃ©grer dans RAGPipeline**
  - Modifier `backend/app/services/rag_pipeline.py`
  - Appeler ConversationManager au lieu de la logique directe
  - Garder la rÃ©trocompatibilitÃ©

- [ ] **4.3 Modifier l'API pour passer le session_id**
  - Fichier: `backend/app/api/routes.py`
  - Ajouter `session_id` dans la requÃªte chat
  - Le frontend doit envoyer un session_id unique par conversation

---

### PHASE 5: TESTS & VALIDATION (PrioritÃ© ğŸŸ¡ MOYENNE)
#### DurÃ©e estimÃ©e: 1-2 heures

- [ ] **5.1 CrÃ©er des tests unitaires**
  - Fichier: `backend/tests/test_intelligent_chat.py`
  - Tests:
    - Test clarification sur message ambigu
    - Test changement de sujet
    - Test continuitÃ© de sujet
    - Test score de confiance

- [ ] **5.2 CrÃ©er des scÃ©narios de test**
  - Fichier: `backend/tests/test_scenarios.py`
  - ScÃ©narios:
    ```python
    SCENARIOS = [
        # Clarification
        {"input": "C'est possible?", "expected": "clarification_question"},
        {"input": "J'ai un problÃ¨me", "expected": "clarification_question"},
        
        # Changement de sujet
        {"flow": [
            {"input": "Quels sont les dÃ©lais?", "topic": "dÃ©lais"},
            {"input": "Et les reliures?", "topic": "reliures", "should_not_mention": "dÃ©lais"}
        ]},
        
        # ContinuitÃ©
        {"flow": [
            {"input": "Quels sont les dÃ©lais?", "topic": "dÃ©lais"},
            {"input": "Et avec l'option express?", "topic": "dÃ©lais", "should_use_context": True}
        ]},
    ]
    ```

- [ ] **5.3 Benchmark de qualitÃ©**
  - Comparer les rÃ©ponses avant/aprÃ¨s sur 20 questions types
  - Mesurer le taux de clarification appropriÃ©
  - Mesurer la qualitÃ© de la gestion des changements de sujet

---

### PHASE 6: OPTIMISATIONS (PrioritÃ© ğŸŸ¢ BASSE)
#### DurÃ©e estimÃ©e: 1 heure

- [ ] **6.1 Cache pour les topics dÃ©tectÃ©s**
  - Ajouter un cache court (5 min) pour Ã©viter de rÃ©analyser

- [ ] **6.2 Optimiser les prompts pour le coÃ»t API**
  - RÃ©duire la taille des prompts de 20%
  - Utiliser des tokens plus efficaces

- [ ] **6.3 Logs et mÃ©triques**
  - Logger les clarifications gÃ©nÃ©rÃ©es
  - Logger les changements de sujet dÃ©tectÃ©s
  - MÃ©triques: % de clarifications, % de changements de sujet

---

## ğŸ“ FICHIERS Ã€ CRÃ‰ER

```
backend/app/services/
â”œâ”€â”€ topic_detector.py          # NOUVEAU - DÃ©tection de sujet
â”œâ”€â”€ context_manager.py         # NOUVEAU - Gestion du contexte
â”œâ”€â”€ clarification_generator.py # NOUVEAU - GÃ©nÃ©ration de clarifications
â”œâ”€â”€ prompt_builder.py          # NOUVEAU - Construction des prompts
â”œâ”€â”€ conversation_manager.py    # NOUVEAU - Orchestrateur principal
â”œâ”€â”€ message_analyzer.py        # MODIFIÃ‰ - Ajout confidence score
â”œâ”€â”€ llm.py                     # MODIFIÃ‰ - Utilise prompt_builder
â””â”€â”€ rag_pipeline.py            # MODIFIÃ‰ - IntÃ¨gre conversation_manager

backend/app/models/
â””â”€â”€ conversation.py            # NOUVEAU - ModÃ¨les de donnÃ©es contexte

backend/tests/
â”œâ”€â”€ test_intelligent_chat.py   # NOUVEAU - Tests unitaires
â””â”€â”€ test_scenarios.py          # NOUVEAU - Tests de scÃ©narios
```

---

## ğŸ“‹ ORDRE D'IMPLÃ‰MENTATION RECOMMANDÃ‰

```
SEMAINE 1 (Core):
â”œâ”€â”€ Jour 1: TopicDetector + ContextManager
â”œâ”€â”€ Jour 2: MessageAnalyzer (confidence) + ClarificationGenerator
â””â”€â”€ Jour 3: PromptBuilder + intÃ©gration llm.py

SEMAINE 2 (Orchestration):
â”œâ”€â”€ Jour 4: ConversationManager
â”œâ”€â”€ Jour 5: IntÃ©gration RAGPipeline + API
â””â”€â”€ Jour 6: Tests + Validation

SEMAINE 3 (Polish):
â”œâ”€â”€ Jour 7: Optimisations + Logs
â””â”€â”€ Jour 8: Benchmark final + Documentation
```

---

## âš ï¸ POINTS D'ATTENTION

1. **CompatibilitÃ© Multi-Provider**
   - Tous les nouveaux services doivent utiliser `self.llm.generate()` ou `self.llm.provider.generate()`
   - Ne PAS appeler directement Ollama ou Mistral
   - Tester avec les 3 providers: `LLM_PROVIDER=mistral`, `groq`, `ollama`

2. **Performance**
   - Le TopicDetector ajoute 1 appel LLM â†’ utiliser un prompt court
   - Cache agressif pour Ã©viter les appels redondants
   - Timeout de 5s sur les analyses

3. **Session Management**
   - Stocker les contextes en mÃ©moire (pas de DB pour l'instant)
   - TTL de 1 heure sur les contextes
   - Nettoyer les contextes expirÃ©s

4. **Fallback**
   - Si TopicDetector Ã©choue â†’ traiter comme "gÃ©nÃ©ral"
   - Si ClarificationGenerator Ã©choue â†’ rÃ©pondre normalement
   - Toujours avoir un comportement par dÃ©faut qui marche

---

## ğŸ¯ CRITÃˆRES DE SUCCÃˆS

| MÃ©trique | Actuel | Cible |
|----------|--------|-------|
| Clarification sur message ambigu | 0% | >80% |
| Changement sujet bien gÃ©rÃ© | ~30% | >90% |
| Contexte utilisÃ© correctement | ~50% | >85% |
| Temps de rÃ©ponse moyen | 2-3s | <3s |
| Satisfaction utilisateur estimÃ©e | 60% | 85% |

---

## ğŸ“ NOTES

- Commencer par Phase 1 et 2 (fondations + clarification)
- Tester aprÃ¨s chaque phase avant de continuer
- Le prompt systÃ¨me est CRUCIAL - itÃ©rer dessus
- Garder l'ancien code comme fallback pendant le dÃ©veloppement

â€ƒ
â€ƒ
â€ƒ
# âœ… TODO - Chatbot Intelligent CoolLibri
# ========================================
# Cocher [x] quand terminÃ©

## PHASE 1: FONDATIONS ğŸ”´ (2-3h)
- [ ] 1.1 `topic_detector.py` - CrÃ©er service dÃ©tection sujet
  - [ ] detect_topic(message) â†’ str
  - [ ] is_topic_change(current, new) â†’ bool
  - [ ] get_topic_category(message) â†’ str
  - [ ] Compatible Ollama/Mistral/Groq

- [ ] 1.2 `context_manager.py` - CrÃ©er gestionnaire contexte
  - [ ] Classe ConversationContext (dataclass)
  - [ ] get_context(session_id) â†’ ConversationContext
  - [ ] update_context(session_id, msg, resp, topic)
  - [ ] get_relevant_context_summary(context, topic) â†’ str
  - [ ] Cache mÃ©moire avec TTL 1h

- [ ] 1.3 `message_analyzer.py` - Ajouter confidence score
  - [ ] Modifier retour: ajouter confidence_score (0-100)
  - [ ] Ajouter ambiguity_type: subject/intent/context/none
  - [ ] Ajouter suggested_clarifications: List[str]
  - [ ] Nouveau prompt LLM pour Ã©valuer ambiguÃ¯tÃ©

## PHASE 2: CLARIFICATION ğŸ”´ (1-2h)
- [ ] 2.1 `clarification_generator.py` - CrÃ©er gÃ©nÃ©rateur
  - [ ] needs_clarification(score, type) â†’ bool (seuil 70%)
  - [ ] generate_clarification(msg, type, ctx) â†’ str
  - [ ] Templates par type d'ambiguÃ¯tÃ©

- [ ] 2.2 DÃ©finir options de clarification
  - [ ] CLARIFICATION_OPTIONS dict par catÃ©gorie
  - [ ] modification: adresse/format/quantitÃ©
  - [ ] problÃ¨me: qualitÃ©/colis/retard
  - [ ] information: formats/dÃ©lais/prix/reliures
  - [ ] commande: suivre/modifier/annuler

## PHASE 3: PROMPT AMÃ‰LIORÃ‰ ğŸ”´ (1h)
- [ ] 3.1 `prompt_builder.py` - CrÃ©er constructeur prompts
  - [ ] build_system_prompt(context) â†’ str
  - [ ] build_user_prompt(query, rag_ctx, conv_ctx) â†’ str
  - [ ] Template dynamique avec injection contexte

- [ ] 3.2 Nouveau prompt systÃ¨me intelligent
  - [ ] Section CONTEXTE CONVERSATION
  - [ ] RÃ¨gle CLARIFICATION (si confiance < 70%)
  - [ ] RÃ¨gle CHANGEMENT DE SUJET
  - [ ] RÃ¨gle UTILISATION DU CONTEXTE
  - [ ] Style: concis, prÃ©cis, professionnel

- [ ] 3.3 Modifier `llm.py`
  - [ ] _get_system_prompt() accepte contexte
  - [ ] generate_response_stream_async() passe contexte

## PHASE 4: ORCHESTRATION ğŸŸ¡ (2h)
- [ ] 4.1 `conversation_manager.py` - CrÃ©er orchestrateur
  - [ ] process_message(session_id, msg, history) â†’ Response
  - [ ] Flux: contexte â†’ analyse â†’ topic â†’ dÃ©cision â†’ rÃ©ponse
  - [ ] IntÃ©grer TopicDetector, ContextManager, Clarification

- [ ] 4.2 Modifier `rag_pipeline.py`
  - [ ] Appeler ConversationManager
  - [ ] Garder rÃ©trocompatibilitÃ©

- [ ] 4.3 Modifier API (`routes.py`)
  - [ ] Ajouter session_id dans requÃªte chat
  - [ ] GÃ©nÃ©rer session_id si absent

## PHASE 5: TESTS ğŸŸ¡ (1-2h)
- [ ] 5.1 `test_intelligent_chat.py` - Tests unitaires
  - [ ] Test clarification message ambigu
  - [ ] Test changement sujet
  - [ ] Test continuitÃ© sujet
  - [ ] Test score confiance

- [ ] 5.2 `test_scenarios.py` - ScÃ©narios
  - [ ] "C'est possible?" â†’ doit clarifier
  - [ ] "dÃ©lais?" puis "reliures?" â†’ change sujet
  - [ ] "dÃ©lais?" puis "option express?" â†’ continue

- [ ] 5.3 Benchmark qualitÃ©
  - [ ] Comparer 20 questions avant/aprÃ¨s
  - [ ] Mesurer % clarifications appropriÃ©es
  - [ ] Mesurer gestion changements sujet

## PHASE 6: OPTIMISATION ğŸŸ¢ (1h)
- [ ] 6.1 Cache topics (TTL 5min)
- [ ] 6.2 RÃ©duire taille prompts -20%
- [ ] 6.3 Logs et mÃ©triques
  - [ ] Log clarifications gÃ©nÃ©rÃ©es
  - [ ] Log changements sujet
  - [ ] MÃ©triques dashboard

## MODÃˆLES DE DONNÃ‰ES
- [ ] `models/conversation.py`
  - [ ] ConversationContext dataclass
  - [ ] TopicSummary dataclass
  - [ ] UserProfile dataclass
  - [ ] ClarificationRequest dataclass

---
# ğŸ“ˆ PROGRESSION
Total: 0/50 items (0%)

Phase 1: 0/12 (0%)
Phase 2: 0/7 (0%)
Phase 3: 0/10 (0%)
Phase 4: 0/7 (0%)
Phase 5: 0/8 (0%)
Phase 6: 0/6 (0%)


