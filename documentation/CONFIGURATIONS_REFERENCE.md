# üéõÔ∏è R√âF√âRENCE RAPIDE - 3 CONFIGURATIONS √Ä TESTER

## Configuration 1Ô∏è‚É£ : PR√âCISION MAXIMALE

**Objectif** : Fid√©lit√© absolue aux sources, z√©ro hallucination

### Fichier `backend/app/core/config.py` (lignes 20-27)
```python
class Settings(BaseSettings):
    # LLM Configuration
    ollama_base_url: str = "http://localhost:11434"
    ollama_model: str = "llama3.1:8b"  # ‚Üê CHANGER LE MOD√àLE
    
    # RAG Configuration
    chunk_size: int = 1000
    chunk_overlap: int = 300
    top_k_results: int = 8      # ‚Üê 8 documents
    rerank_top_n: int = 4        # ‚Üê 4 meilleurs
```

### Fichier `backend/app/services/llm.py` (ligne ~78)
```python
options={
    "temperature": 0.0,          # ‚Üê Z√©ro cr√©ativit√©
    "top_p": 0.3,               # ‚Üê Tr√®s conservateur
    "top_k": 20,                # ‚Üê Vocabulaire restreint
    "num_predict": 800,         # ‚Üê R√©ponses moyennes
    "repeat_penalty": 1.2,
}
```

**Cas d'usage** : Questions avec chiffres pr√©cis, donn√©es critiques

---

## Configuration 2Ô∏è‚É£ : √âQUILIBR√âE (RECOMMAND√âE)

**Objectif** : Bon compromis pr√©cision/fluidit√©

### Fichier `backend/app/core/config.py` (lignes 20-27)
```python
class Settings(BaseSettings):
    # LLM Configuration
    ollama_base_url: str = "http://localhost:11434"
    ollama_model: str = "llama3.1:8b"  # ‚Üê CHANGER LE MOD√àLE
    
    # RAG Configuration
    chunk_size: int = 1000
    chunk_overlap: int = 300
    top_k_results: int = 10      # ‚Üê 10 documents
    rerank_top_n: int = 5        # ‚Üê 5 meilleurs
```

### Fichier `backend/app/services/llm.py` (ligne ~78)
```python
options={
    "temperature": 0.15,         # ‚Üê Tr√®s l√©g√®re variation
    "top_p": 0.5,               # ‚Üê √âquilibr√©
    "top_k": 40,                # ‚Üê Vocabulaire riche
    "num_predict": 900,         # ‚Üê R√©ponses d√©taill√©es
    "repeat_penalty": 1.2,
}
```

**Cas d'usage** : Usage quotidien chatbot, questions vari√©es

---

## Configuration 3Ô∏è‚É£ : R√âPONSES COMPL√àTES

**Objectif** : R√©ponses d√©taill√©es et exhaustives

### Fichier `backend/app/core/config.py` (lignes 20-27)
```python
class Settings(BaseSettings):
    # LLM Configuration
    ollama_base_url: str = "http://localhost:11434"
    ollama_model: str = "llama3.1:8b"  # ‚Üê CHANGER LE MOD√àLE
    
    # RAG Configuration
    chunk_size: int = 1000
    chunk_overlap: int = 300
    top_k_results: int = 12      # ‚Üê 12 documents
    rerank_top_n: int = 6        # ‚Üê 6 meilleurs
```

### Fichier `backend/app/services/llm.py` (ligne ~78)
```python
options={
    "temperature": 0.2,          # ‚Üê L√©g√®re cr√©ativit√©
    "top_p": 0.6,               # ‚Üê Plus de diversit√©
    "top_k": 50,                # ‚Üê Vocabulaire tr√®s riche
    "num_predict": 1200,        # ‚Üê R√©ponses longues
    "repeat_penalty": 1.2,
}
```

**Cas d'usage** : Questions complexes, explications d√©taill√©es

---

## üìã TABLEAU R√âCAPITULATIF

| Param√®tre | Config 1 (Pr√©cision) | Config 2 (√âquilibr√©e) | Config 3 (Compl√®te) |
|-----------|----------------------|-----------------------|---------------------|
| **temperature** | 0.0 | 0.15 | 0.2 |
| **top_p** | 0.3 | 0.5 | 0.6 |
| **top_k** | 20 | 40 | 50 |
| **num_predict** | 800 | 900 | 1200 |
| **top_k_results** | 8 | 10 | 12 |
| **rerank_top_n** | 4 | 5 | 6 |

---

## üîÑ PROC√âDURE RAPIDE

1. **Copier-coller** la configuration dans les 2 fichiers
2. **Sauvegarder** les fichiers
3. **Red√©marrer** le backend :
   ```powershell
   cd backend
   .\.venv\Scripts\Activate.ps1
   uvicorn main:app --reload --host 0.0.0.0 --port 8080
   ```
4. **Tester** les 30 questions
5. **Passer** √† la configuration suivante

---

## üéØ ORDRE DE TEST RECOMMAND√â

### Pour chaque mod√®le (llama3.1:8b, llama3.2:3b, mistral:7b, phi3:medium) :

1. **Config 2 (√âquilibr√©e)** - Tester en premier (baseline)
2. **Config 1 (Pr√©cision)** - Comparer avec baseline
3. **Config 3 (Compl√®te)** - Comparer avec baseline

Cela permet de voir rapidement si les extr√™mes am√©liorent ou d√©gradent les r√©sultats.

---

**‚ö†Ô∏è IMPORTANT** : Toujours red√©marrer le backend apr√®s chaque modification !
