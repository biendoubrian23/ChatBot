# ğŸš€ QUE LANCE EXACTEMENT LE BACKEND ?

## 1ï¸âƒ£ COMMANDE QUE TU EXÃ‰CUTES

```powershell
cd X:\MesApplis\BiendouCorp\ChatBot\backend
.\.venv\Scripts\Activate.ps1
uvicorn main:app --reload --host 0.0.0.0 --port 8080
```

---

## 2ï¸âƒ£ Ã‰TAPE PAR Ã‰TAPE : QU'EST-CE QUI SE PASSE ?

### âœ… Ã‰tape 1 : Activation de l'environnement virtuel

```powershell
.\.venv\Scripts\Activate.ps1
```

**Action :**
- Charge l'environnement Python isolÃ© (.venv)
- Active les dÃ©pendances (FastAPI, Ollama, ChromaDB, etc.)
- Change le prompt en `(.venv) PS X:\...`

---

### âœ… Ã‰tape 2 : Lancement d'Uvicorn

```powershell
uvicorn main:app --reload --host 0.0.0.0 --port 8080
```

**DÃ©codage :**
- `uvicorn` = Serveur ASGI (Web server pour Python async)
- `main:app` = Fichier `main.py`, objet `app` (FastAPI app)
- `--reload` = Mode dÃ©veloppement (recharge auto si changements)
- `--host 0.0.0.0` = Ã‰coute SUR TOUTES LES INTERFACES (localhost + IP locale + Internet)
- `--port 8080` = Port d'Ã©coute

---

## 3ï¸âƒ£ CE QUI SE LANCE RÃ‰ELLEMENT

### ğŸ“Š Processus de dÃ©marrage

```
Uvicorn dÃ©marre
    â†“
Charge le fichier main.py
    â†“
CrÃ©e l'objet FastAPI (app)
    â†“
DÃ©clenche @app.on_event("startup")
    â”œâ”€ Charge EmbeddingService
    â”‚   â””â”€ Charge modÃ¨le SentenceTransformers (~500MB)
    â”‚      â””â”€ "paraphrase-multilingual-mpnet-base-v2"
    â”‚
    â”œâ”€ Charge VectorStoreService
    â”‚   â””â”€ Charge ChromaDB
    â”‚      â””â”€ Charge vectorstore depuis ./data/vectorstore/
    â”‚         â””â”€ chroma.sqlite3 (8252 documents vectorisÃ©s)
    â”‚
    â”œâ”€ Charge OllamaService
    â”‚   â””â”€ Se connecte Ã  Ollama (http://localhost:11434)
    â”‚      â””â”€ VÃ©rifie disponibilitÃ© du modÃ¨le llama3.1:8b
    â”‚
    â””â”€ CrÃ©e RAGPipeline
        â””â”€ Combine tout ensemble
    
    â†“
Uvicorn Ã©coute sur 0.0.0.0:8080
    â†“
"âœ… LibriAssist API is ready!"
```

---

## 4ï¸âƒ£ Ã€ QUOI RESSEMBLE LE OUTPUT AU DÃ‰MARRAGE ?

```
INFO:     Uvicorn running on http://0.0.0.0:8080 (Press CTRL+C to quit)
INFO:     Started reloader process [20576] using WatchFiles

ğŸš€ Starting LibriAssist API...
ğŸ“¦ Version: 1.0.0

ğŸ”§ Initializing services...

Loading embedding model: paraphrase-multilingual-mpnet-base-v2
âœ“ Embedding model loaded successfully

âœ“ Vector store initialized with 8252 documents

âœ“ Ollama service is available

âœ… LibriAssist API is ready!
ğŸ“ Listening on http://0.0.0.0:8080
ğŸ“š Vector store contains 8252 documents

ğŸ’¡ Tip: Use /docs for API documentation
```

---

## 5ï¸âƒ£ CE QUI EST MAINTENANT ACCESSIBLE

### ğŸ“¡ En local (sur ta machine)

```
http://localhost:8080                    # Page d'accueil
http://localhost:8080/api/v1/chat        # Endpoint chat
http://localhost:8080/docs                # Documentation Swagger
http://127.0.0.1:8080                    # Localhost (alias)
```

### ğŸŒ Sur ton rÃ©seau local

```
http://192.168.1.100:8080/               # Si ta machine a cette IP
http://[IP_DE_TA_MACHINE]:8080/          # Depuis un autre PC
```

### âŒ Depuis Internet

```
âŒ http://ta-ip-publique:8080/           # Ne fonctionne PAS (routeur bloque)
âŒ https://ta-ip-publique:8080/          # Ne fonctionne PAS (pas HTTPS)
```

---

## 6ï¸âƒ£ C'EST LÃ€ QUE NGROK INTERVIENT

### ğŸ”— NGROK = Tunnel Internet

```powershell
ngrok http 8080
```

**Qu'est-ce que Ã§a fait :**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ta machine (X:\MesApplis\BiendouCorp\ChatBot)         â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Backend FastAPI                                â”‚  â”‚
â”‚  â”‚  Uvicorn: 0.0.0.0:8080                         â”‚  â”‚
â”‚  â”‚  âœ… Ã‰coute sur port 8080                       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚           â†‘                                            â”‚
â”‚           â”‚ localhost:8080                            â”‚
â”‚           â”‚                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  NGROK Agent (process)                          â”‚  â”‚
â”‚  â”‚  - CrÃ©e connexion sortante vers ngrok.io       â”‚  â”‚
â”‚  â”‚  - Obtient URL temporaire                       â”‚  â”‚
â”‚  â”‚  - CrÃ©e tunnel bidirectionnel                   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”‚ Tunnel chiffrÃ© HTTPS
           â”‚
           â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   ngrok.io servers   â”‚
    â”‚   (Serveurs publics) â”‚
    â”‚                      â”‚
    â”‚ https://XXXX.ngrok... â”‚  â† URL gÃ©nÃ©rÃ©e dynamiquement
    â”‚ (change chaque fois) â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”‚
           â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Internet Public    â”‚
    â”‚                      â”‚
    â”‚  Accessible PARTOUT  â”‚
    â”‚  CoolLibri, Widget,  â”‚
    â”‚  n'importe oÃ¹ !      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 7ï¸âƒ£ OUTPUT DE NGROK AU DÃ‰MARRAGE

```
ngrok                                                  (Ctrl+C to quit)

Session Status                online                                                                                      
Account                       Free                                                                                        
Version                        3.3.0                                                                                       
Region                         Europe (eu)                                                                                 
Latency                        25ms                                                                                        
Web Interface                  http://127.0.0.1:4040                                                                       
Forwarding                     https://8f4a-2001-0db8-85a3-0000-0000-8a2e-0370-1234.eu.ngrok.io -> http://localhost:8080  
                                                                                                                            
Connections                    ttl     opn     rt1     rt5     p50     p90                                                
                                0       0       0.00    0.00    0.00    0.00  
```

**C'est quoi :**
- `Forwarding`: URL publique â†’ localhost:8080
- `Web Interface`: http://127.0.0.1:4040 (dashboard NGROK local)
- **L'URL change Ã  chaque redÃ©marrage !**

---

## 8ï¸âƒ£ COMMENT Ã‡A FONCTIONNE EN DÃ‰TAIL

### ğŸ”„ Flux de requÃªte via NGROK

```
1ï¸âƒ£ CLIENT (Widget sur CoolLibri)
   Demande: https://8f4a-...ngrok.io/api/v1/chat
   Body: {"question": "OÃ¹ en est ma commande ?"}
       â”‚
       â†“
2ï¸âƒ£ NGROK Serveur Public (ngrok.io)
   ReÃ§oit la requÃªte
   Enregistre dans logs
       â”‚
       â†“
3ï¸âƒ£ NGROK Client Local (sur ta machine)
   ReÃ§oit la requÃªte via tunnel chiffrÃ©
   Envoie Ã  http://localhost:8080/api/v1/chat
       â”‚
       â†“
4ï¸âƒ£ BACKEND FastAPI (Uvicorn)
   ReÃ§oit sur port 8080
   Traite: RAG Pipeline â†’ Ollama â†’ RÃ©ponse
       â”‚
       â†“
5ï¸âƒ£ RÃ©ponse remonte
   FastAPI â†’ NGROK Client â†’ NGROK Serveur â†’ Client
       â”‚
       â†“
6ï¸âƒ£ CLIENT reÃ§oit la rÃ©ponse
   Widget affiche la rÃ©ponse au client CoolLibri
```

---

## 9ï¸âƒ£ LE LLM (Ollama) N'EST PAS SUR NGROK

### âŒ IMPORTANT : Ollama n'est PAS sur Internet

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ta machine                             â”‚
â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Backend FastAPI (port 8080)     â”‚  â”‚  â† Sur NGROK
â”‚  â”‚  Uvicorn                         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚               â”‚ RequÃªte: localhost:... â”‚
â”‚               â†“                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Ollama (port 11434)             â”‚  â”‚  â† PAS sur NGROK
â”‚  â”‚  ModÃ¨le: llama3.1:8b             â”‚  â”‚     (local uniquement)
â”‚  â”‚  GPU: CUDA/Metal                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†‘
         â”‚ NGROK tunnel
         â†“
    Internet public
    (Clients CoolLibri)
```

---

## ğŸ”Ÿ ARCHITECTURE COMPLÃˆTE

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SITE COOLLIBRI (Internet)                     â”‚
â”‚                    Widget ChatBot                                 â”‚
â”‚                    (Client Vue/JavaScript)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚ HTTPS Request
                  â”‚ https://8f4a-...ngrok.io/api/v1/chat
                  â”‚
                  â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   NGROK Public URL      â”‚
        â”‚ (https://XXXX.ngrok...) â”‚
        â”‚ Region: eu              â”‚
        â”‚ Session: active         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ Tunnel chiffrÃ©
                       â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚   Ta Machine (local)          â”‚
       â”‚                               â”‚
       â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
       â”‚  â”‚  NGROK Agent            â”‚  â”‚
       â”‚  â”‚  Port: 8080 â†â†’ Internet â”‚  â”‚
       â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
       â”‚             â”‚ localhost:8080   â”‚
       â”‚             â†“                  â”‚
       â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
       â”‚  â”‚  FastAPI + Uvicorn      â”‚  â”‚
       â”‚  â”‚  Port: 8080             â”‚  â”‚
       â”‚  â”‚  - Route /api/v1/chat   â”‚  â”‚
       â”‚  â”‚  - CORS configured      â”‚  â”‚
       â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
       â”‚             â”‚ RAG Pipeline     â”‚
       â”‚             â†“                  â”‚
       â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
       â”‚  â”‚  Services               â”‚  â”‚
       â”‚  â”‚  - EmbeddingService     â”‚  â”‚
       â”‚  â”‚  - VectorStore (ChromaDB)   â”‚
       â”‚  â”‚  - OllamaService        â”‚  â”‚
       â”‚  â”‚  - RAGPipeline          â”‚  â”‚
       â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
       â”‚             â”‚ localhost:11434 â”‚
       â”‚             â†“                  â”‚
       â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
       â”‚  â”‚  Ollama                 â”‚  â”‚
       â”‚  â”‚  Port: 11434 (local)    â”‚  â”‚
       â”‚  â”‚  Model: llama3.1:8b     â”‚  â”‚
       â”‚  â”‚  GPU Processing         â”‚  â”‚
       â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
       â”‚                               â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1ï¸âƒ£1ï¸âƒ£ POUR RÃ‰SUMER

### Commande `uvicorn main:app --reload --host 0.0.0.0 --port 8080`

**Lance :**
1. âœ… **FastAPI app** sur port 8080
2. âœ… **Services** : EmbeddingService, VectorStore, Ollama, RAGPipeline
3. âœ… **Endpoints** : /api/v1/chat, /docs, etc.
4. âœ… **CORS** : Configure les origines acceptÃ©es
5. âœ… **Accessible** : localhost:8080 + rÃ©seau local

### Commande `ngrok http 8080`

**CrÃ©e :**
1. âœ… **Tunnel public** depuis Internet â†’ localhost:8080
2. âœ… **URL temporaire** : https://XXXX-XXXX.ngrok.io
3. âœ… **Chiffrement HTTPS** automatique
4. âœ… **AccÃ¨s** : Depuis CoolLibri (Internet) vers ton backend (local)

### âš ï¸ Ollama (LLM)

- **N'est PAS sur NGROK** (resterait local)
- âœ… Communique en local: localhost:11434
- âœ… Processus GPU intensif (pas besoin d'Ãªtre sur Internet)
- âœ… Le backend (NGROK) appelle Ollama en interne

---

## ğŸ¯ FLUX COMPLET D'UNE QUESTION

```
1. Client CoolLibri tape: "OÃ¹ est ma commande ?"
                          â”‚
                          â†“ HTTPS (NGROK)
2. Backend reÃ§oit sur http://localhost:8080/api/v1/chat
                          â”‚
                          â†“ Traitement
3. RAGPipeline analyse question
   - Vectorize question (EmbeddingService)
   - Search ChromaDB (VectorStore)
   - Format context
                          â”‚
                          â†“ RequÃªte locale
4. Appel Ollama sur http://localhost:11434
   - Envoie: context + question + system_prompt
   - GÃ©nÃ¨re rÃ©ponse avec llama3.1:8b
                          â”‚
                          â†“ RÃ©ponse
5. Backend retourne rÃ©ponse
                          â”‚
                          â†“ HTTPS (NGROK)
6. Client reÃ§oit rÃ©ponse et l'affiche
```

---

**Des questions sur une partie spÃ©cifique ?** ğŸš€
