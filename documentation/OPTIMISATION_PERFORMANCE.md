# ðŸš€ Guide d'Optimisation des Performances - LibriAssist

## Table des MatiÃ¨res
1. [Situation Actuelle](#situation-actuelle)
2. [Optimisations ImmÃ©diates (PC Local)](#optimisations-immÃ©diates-pc-local)
3. [Configuration Serveur DÃ©diÃ©](#configuration-serveur-dÃ©diÃ©)
4. [Modifications du Code](#modifications-du-code)
5. [Architecture Cible pour 100-200 Utilisateurs](#architecture-cible)

---

## Situation Actuelle

### Architecture Existante
```
[Utilisateur] â†’ [FastAPI] â†’ [ChromaDB] â†’ [Ollama (mistral:7b)]
                    â†“
              [1 instance]
```

### ProblÃ¨me Principal
- **Ollama traite 1 requÃªte Ã  la fois**
- Temps moyen par rÃ©ponse : 3-8 secondes
- **DÃ©bit maximum : ~10-15 requÃªtes/minute**

### ConsÃ©quence
| Utilisateurs SimultanÃ©s | Temps d'Attente Moyen |
|------------------------|----------------------|
| 1 | 5 secondes |
| 10 | 50 secondes |
| 100 | 8+ minutes |
| 200 | 16+ minutes |

---

## Optimisations ImmÃ©diates (PC Local)

### 1. Configurer Ollama pour le ParallÃ©lisme

Ollama supporte nativement le parallÃ©lisme avec les variables d'environnement :

#### Windows (PowerShell)
```powershell
# DÃ©finir les variables d'environnement avant de lancer Ollama
$env:OLLAMA_NUM_PARALLEL = "3"        # 3 requÃªtes en parallÃ¨le
$env:OLLAMA_MAX_LOADED_MODELS = "1"   # 1 modÃ¨le en mÃ©moire (Ã©conomie RAM)

# Lancer Ollama
ollama serve
```

#### CrÃ©er un script de dÃ©marrage `start_ollama_parallel.ps1`
```powershell
# start_ollama_parallel.ps1
Write-Host "ðŸš€ DÃ©marrage d'Ollama avec parallÃ©lisme activÃ©..."

# Configuration pour 2-3 utilisateurs simultanÃ©s
$env:OLLAMA_NUM_PARALLEL = "3"
$env:OLLAMA_MAX_LOADED_MODELS = "1"
$env:OLLAMA_KEEP_ALIVE = "5m"

# Optionnel : limiter la mÃ©moire si besoin
# $env:OLLAMA_MAX_VRAM = "4096"  # 4GB max

Write-Host "Configuration:"
Write-Host "  - RequÃªtes parallÃ¨les: $env:OLLAMA_NUM_PARALLEL"
Write-Host "  - ModÃ¨les en mÃ©moire: $env:OLLAMA_MAX_LOADED_MODELS"

ollama serve
```

### 2. Utiliser un ModÃ¨le Plus Rapide

#### Option A : ModÃ¨le QuantifiÃ© (RecommandÃ©)
```powershell
# TÃ©lÃ©charger la version quantifiÃ©e (2x plus rapide, qualitÃ© similaire)
ollama pull mistral:7b-instruct-q4_0
```

Modifier `config.py` :
```python
OLLAMA_MODEL = "mistral:7b-instruct-q4_0"  # Au lieu de "mistral:7b"
```

#### Option B : ModÃ¨le Plus Petit
```powershell
# Phi-3 Mini - 3.8B paramÃ¨tres, trÃ¨s rapide
ollama pull phi3:mini
```

### 3. RÃ©duire la Taille des RÃ©ponses

Dans `llm.py`, rÃ©duire `num_predict` :
```python
options={
    "temperature": 0.1,
    "num_predict": 200,  # Au lieu de 400 (2x plus rapide)
    "top_k": 20,
}
```

### 4. Lancer Uvicorn avec Plusieurs Workers

```powershell
# Au lieu de : python -m uvicorn main:app --host 0.0.0.0 --port 8000
# Utiliser :
python -m uvicorn main:app --host 0.0.0.0 --port 8000 --workers 2
```

### RÃ©sultat Attendu (PC Local)

| Configuration | Utilisateurs ParallÃ¨les | Temps RÃ©ponse |
|--------------|------------------------|---------------|
| Actuelle | 1 | 5s |
| OptimisÃ©e | 2-3 | 5-7s chacun |

---

## Configuration Serveur DÃ©diÃ©

### SpÃ©cifications RecommandÃ©es

#### Pour 100 Utilisateurs SimultanÃ©s
| Composant | Minimum | RecommandÃ© |
|-----------|---------|------------|
| CPU | 16 cores | 32 cores |
| RAM | 32 GB | 64 GB |
| GPU | RTX 3080 (10GB) | RTX 4090 (24GB) ou A100 |
| SSD | 500 GB NVMe | 1 TB NVMe |
| RÃ©seau | 1 Gbps | 10 Gbps |

#### Pour 200 Utilisateurs SimultanÃ©s
| Composant | Minimum | RecommandÃ© |
|-----------|---------|------------|
| CPU | 32 cores | 64 cores |
| RAM | 64 GB | 128 GB |
| GPU | 2x RTX 4090 | A100 80GB ou H100 |
| SSD | 1 TB NVMe | 2 TB NVMe RAID |
| RÃ©seau | 10 Gbps | 25 Gbps |

### Architecture Serveur RecommandÃ©e

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚              NGINX                       â”‚
                    â”‚         (Load Balancer)                  â”‚
                    â”‚        Rate Limiting                     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                          â”‚                          â”‚
        â–¼                          â–¼                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI    â”‚         â”‚   FastAPI    â”‚         â”‚   FastAPI    â”‚
â”‚  Worker 1    â”‚         â”‚  Worker 2    â”‚         â”‚  Worker 3    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                        â”‚                        â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚        Redis          â”‚
                    â”‚   (Cache SÃ©mantique)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                       â”‚                       â”‚
        â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Ollama 1   â”‚       â”‚   Ollama 2   â”‚       â”‚   Ollama 3   â”‚
â”‚  (GPU 0)     â”‚       â”‚  (GPU 1)     â”‚       â”‚  (CPU/GPU 2) â”‚
â”‚  Port 11434  â”‚       â”‚  Port 11435  â”‚       â”‚  Port 11436  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Installation Serveur Linux (Ubuntu 22.04)

#### 1. Installation de Base
```bash
# Mise Ã  jour systÃ¨me
sudo apt update && sudo apt upgrade -y

# Installer Docker et Docker Compose
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
sudo apt install docker-compose -y

# Installer NVIDIA Container Toolkit (si GPU)
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list
sudo apt update
sudo apt install nvidia-container-toolkit -y
sudo systemctl restart docker
```

#### 2. Docker Compose pour Multi-Instances Ollama

CrÃ©er `docker-compose.yml` :
```yaml
version: '3.8'

services:
  # Load Balancer
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - api1
      - api2
      - api3

  # Redis Cache
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes

  # API Instances
  api1:
    build: ./backend
    environment:
      - REDIS_URL=redis://redis:6379
      - OLLAMA_URLS=http://ollama1:11434,http://ollama2:11434,http://ollama3:11434
    depends_on:
      - redis
      - ollama1
      - ollama2
      - ollama3

  api2:
    build: ./backend
    environment:
      - REDIS_URL=redis://redis:6379
      - OLLAMA_URLS=http://ollama1:11434,http://ollama2:11434,http://ollama3:11434
    depends_on:
      - redis
      - ollama1
      - ollama2
      - ollama3

  api3:
    build: ./backend
    environment:
      - REDIS_URL=redis://redis:6379
      - OLLAMA_URLS=http://ollama1:11434,http://ollama2:11434,http://ollama3:11434
    depends_on:
      - redis
      - ollama1
      - ollama2
      - ollama3

  # Ollama Instances
  ollama1:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_NUM_PARALLEL=4
      - OLLAMA_MAX_LOADED_MODELS=1
    volumes:
      - ollama1_data:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]

  ollama2:
    image: ollama/ollama:latest
    ports:
      - "11435:11434"
    environment:
      - OLLAMA_NUM_PARALLEL=4
      - OLLAMA_MAX_LOADED_MODELS=1
    volumes:
      - ollama2_data:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['1']
              capabilities: [gpu]

  ollama3:
    image: ollama/ollama:latest
    ports:
      - "11436:11434"
    environment:
      - OLLAMA_NUM_PARALLEL=4
      - OLLAMA_MAX_LOADED_MODELS=1
    volumes:
      - ollama3_data:/root/.ollama

volumes:
  redis_data:
  ollama1_data:
  ollama2_data:
  ollama3_data:
```

#### 3. Configuration NGINX

CrÃ©er `nginx.conf` :
```nginx
events {
    worker_connections 1024;
}

http {
    upstream api_servers {
        least_conn;  # Envoie vers le serveur le moins chargÃ©
        server api1:8000;
        server api2:8000;
        server api3:8000;
    }

    # Rate limiting
    limit_req_zone $binary_remote_addr zone=api_limit:10m rate=10r/s;
    limit_conn_zone $binary_remote_addr zone=conn_limit:10m;

    server {
        listen 80;
        server_name _;

        # Limites
        limit_req zone=api_limit burst=20 nodelay;
        limit_conn conn_limit 10;

        location / {
            proxy_pass http://api_servers;
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection 'upgrade';
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_cache_bypass $http_upgrade;
            
            # Timeout pour le streaming
            proxy_read_timeout 120s;
            proxy_send_timeout 120s;
        }

        # SSE Streaming
        location /api/v1/chat/stream {
            proxy_pass http://api_servers;
            proxy_http_version 1.1;
            proxy_set_header Connection '';
            proxy_buffering off;
            proxy_cache off;
            chunked_transfer_encoding off;
            proxy_read_timeout 300s;
        }
    }
}
```

---

## Modifications du Code

### 1. Load Balancer pour Ollama (Multi-Instance)

CrÃ©er `backend/app/services/ollama_pool.py` :
```python
"""Pool de connexions Ollama avec load balancing."""
import ollama
import asyncio
from typing import List, Optional
from dataclasses import dataclass
from collections import deque
import time


@dataclass
class OllamaInstance:
    """Une instance Ollama."""
    url: str
    client: ollama.Client
    current_requests: int = 0
    last_used: float = 0
    is_healthy: bool = True


class OllamaPool:
    """Pool de connexions Ollama avec load balancing."""
    
    def __init__(self, urls: List[str], model: str = "mistral:7b"):
        """
        Args:
            urls: Liste des URLs Ollama (ex: ["http://localhost:11434", "http://localhost:11435"])
            model: ModÃ¨le Ã  utiliser
        """
        self.model = model
        self.instances: List[OllamaInstance] = []
        self._lock = asyncio.Lock()
        
        for url in urls:
            client = ollama.Client(host=url)
            self.instances.append(OllamaInstance(url=url, client=client))
        
        print(f"ðŸ”— OllamaPool initialisÃ© avec {len(self.instances)} instances")
    
    async def get_best_instance(self) -> OllamaInstance:
        """Retourne l'instance la moins chargÃ©e."""
        async with self._lock:
            # Filtrer les instances saines
            healthy = [i for i in self.instances if i.is_healthy]
            
            if not healthy:
                # RÃ©essayer toutes les instances
                healthy = self.instances
            
            # Trier par nombre de requÃªtes en cours
            healthy.sort(key=lambda x: x.current_requests)
            
            best = healthy[0]
            best.current_requests += 1
            best.last_used = time.time()
            
            return best
    
    async def release_instance(self, instance: OllamaInstance):
        """LibÃ¨re une instance aprÃ¨s utilisation."""
        async with self._lock:
            instance.current_requests = max(0, instance.current_requests - 1)
    
    async def mark_unhealthy(self, instance: OllamaInstance):
        """Marque une instance comme non saine."""
        async with self._lock:
            instance.is_healthy = False
            # RÃ©activer aprÃ¨s 30 secondes
            asyncio.create_task(self._reactivate_after(instance, 30))
    
    async def _reactivate_after(self, instance: OllamaInstance, seconds: int):
        """RÃ©active une instance aprÃ¨s un dÃ©lai."""
        await asyncio.sleep(seconds)
        async with self._lock:
            instance.is_healthy = True
            print(f"âœ… Instance {instance.url} rÃ©activÃ©e")
    
    async def generate_stream(self, prompt: str, system: str, options: dict):
        """GÃ©nÃ¨re une rÃ©ponse en streaming avec load balancing."""
        instance = await self.get_best_instance()
        
        try:
            stream = instance.client.generate(
                model=self.model,
                prompt=prompt,
                system=system,
                stream=True,
                options=options
            )
            
            for chunk in stream:
                if 'response' in chunk:
                    yield chunk['response']
                    
        except Exception as e:
            print(f"âŒ Erreur sur {instance.url}: {e}")
            await self.mark_unhealthy(instance)
            raise
        finally:
            await self.release_instance(instance)
    
    async def health_check(self) -> dict:
        """VÃ©rifie la santÃ© de toutes les instances."""
        results = {}
        for instance in self.instances:
            try:
                instance.client.list()
                results[instance.url] = {
                    "healthy": True,
                    "current_requests": instance.current_requests
                }
            except Exception as e:
                results[instance.url] = {
                    "healthy": False,
                    "error": str(e)
                }
        return results
```

### 2. Cache SÃ©mantique avec Redis

CrÃ©er `backend/app/services/semantic_cache.py` :
```python
"""Cache sÃ©mantique pour les rÃ©ponses frÃ©quentes."""
import redis
import json
import hashlib
from typing import Optional
import numpy as np
from sentence_transformers import SentenceTransformer


class SemanticCache:
    """Cache sÃ©mantique basÃ© sur la similaritÃ© des questions."""
    
    def __init__(
        self, 
        redis_url: str = "redis://localhost:6379",
        similarity_threshold: float = 0.92,
        ttl_seconds: int = 3600  # 1 heure
    ):
        self.redis = redis.from_url(redis_url)
        self.similarity_threshold = similarity_threshold
        self.ttl = ttl_seconds
        self.encoder = None  # ChargÃ© Ã  la demande
        
    def _get_encoder(self):
        """Charge l'encodeur si nÃ©cessaire."""
        if self.encoder is None:
            # Utiliser le mÃªme modÃ¨le que le vectorstore
            self.encoder = SentenceTransformer('intfloat/multilingual-e5-large')
        return self.encoder
    
    def _get_embedding(self, text: str) -> np.ndarray:
        """Calcule l'embedding d'un texte."""
        encoder = self._get_encoder()
        return encoder.encode(f"query: {text}", normalize_embeddings=True)
    
    def _cosine_similarity(self, a: np.ndarray, b: np.ndarray) -> float:
        """Calcule la similaritÃ© cosinus."""
        return float(np.dot(a, b))
    
    def _hash_question(self, question: str) -> str:
        """CrÃ©e un hash de la question."""
        return hashlib.sha256(question.lower().strip().encode()).hexdigest()[:16]
    
    async def get(self, question: str) -> Optional[str]:
        """
        Cherche une rÃ©ponse en cache pour une question similaire.
        
        Returns:
            La rÃ©ponse cachÃ©e ou None si pas trouvÃ©e
        """
        try:
            # 1. VÃ©rifier le cache exact (trÃ¨s rapide)
            exact_key = f"exact:{self._hash_question(question)}"
            exact_result = self.redis.get(exact_key)
            if exact_result:
                return json.loads(exact_result)["response"]
            
            # 2. VÃ©rifier le cache sÃ©mantique
            question_embedding = self._get_embedding(question)
            
            # RÃ©cupÃ©rer toutes les clÃ©s sÃ©mantiques
            semantic_keys = self.redis.keys("semantic:*")
            
            for key in semantic_keys[:50]:  # Limiter Ã  50 pour la performance
                data = self.redis.get(key)
                if data:
                    cached = json.loads(data)
                    cached_embedding = np.array(cached["embedding"])
                    
                    similarity = self._cosine_similarity(question_embedding, cached_embedding)
                    
                    if similarity >= self.similarity_threshold:
                        # Toucher le TTL pour les hits frÃ©quents
                        self.redis.expire(key, self.ttl)
                        return cached["response"]
            
            return None
            
        except Exception as e:
            print(f"Cache error (get): {e}")
            return None
    
    async def set(self, question: str, response: str):
        """
        Stocke une rÃ©ponse en cache.
        
        Args:
            question: La question posÃ©e
            response: La rÃ©ponse gÃ©nÃ©rÃ©e
        """
        try:
            # 1. Cache exact
            exact_key = f"exact:{self._hash_question(question)}"
            self.redis.setex(
                exact_key,
                self.ttl,
                json.dumps({"response": response})
            )
            
            # 2. Cache sÃ©mantique
            embedding = self._get_embedding(question)
            semantic_key = f"semantic:{self._hash_question(question)}"
            
            self.redis.setex(
                semantic_key,
                self.ttl,
                json.dumps({
                    "question": question,
                    "response": response,
                    "embedding": embedding.tolist()
                })
            )
            
        except Exception as e:
            print(f"Cache error (set): {e}")
    
    def get_stats(self) -> dict:
        """Retourne les statistiques du cache."""
        try:
            exact_count = len(self.redis.keys("exact:*"))
            semantic_count = len(self.redis.keys("semantic:*"))
            
            return {
                "exact_entries": exact_count,
                "semantic_entries": semantic_count,
                "total_entries": exact_count + semantic_count
            }
        except Exception as e:
            return {"error": str(e)}
```

### 3. IntÃ©gration dans le Service LLM

Modifier `backend/app/services/llm.py` pour utiliser le pool et le cache :
```python
# Ajouter en haut du fichier
from app.services.ollama_pool import OllamaPool
from app.services.semantic_cache import SemanticCache

class OllamaService:
    def __init__(
        self, 
        base_url: str = "http://localhost:11434",
        model: str = "mistral:7b",
        ollama_urls: list = None,  # Pour multi-instance
        redis_url: str = None       # Pour le cache
    ):
        self.model = model
        
        # Multi-instance ou single instance
        if ollama_urls and len(ollama_urls) > 1:
            self.pool = OllamaPool(ollama_urls, model)
            self.client = None
        else:
            self.pool = None
            self.client = ollama.Client(host=base_url)
        
        # Cache sÃ©mantique (optionnel)
        self.cache = SemanticCache(redis_url) if redis_url else None
    
    async def generate_with_cache(self, query: str, context: str, history=None):
        """GÃ©nÃ¨re avec cache sÃ©mantique."""
        # 1. VÃ©rifier le cache
        if self.cache:
            cached = await self.cache.get(query)
            if cached:
                print(f"âœ… Cache HIT pour: {query[:50]}...")
                return cached
        
        # 2. GÃ©nÃ©rer la rÃ©ponse
        response = await self._generate(query, context, history)
        
        # 3. Stocker en cache
        if self.cache:
            await self.cache.set(query, response)
        
        return response
```

---

## Architecture Cible

### Pour 100 Utilisateurs SimultanÃ©s

| Composant | Instances | Configuration |
|-----------|-----------|---------------|
| NGINX | 1 | Rate limit 10 req/s/user |
| FastAPI | 3 | 2 workers chacun |
| Redis | 1 | 2GB RAM, AOF |
| Ollama | 3 | NUM_PARALLEL=4 chacun |
| **Total GPU** | 1-2 | RTX 4090 ou Ã©quivalent |

**DÃ©bit estimÃ© :** 50-80 requÃªtes/minute (avec cache: 150+/min)

### Pour 200 Utilisateurs SimultanÃ©s

| Composant | Instances | Configuration |
|-----------|-----------|---------------|
| NGINX | 2 (HA) | Rate limit 5 req/s/user |
| FastAPI | 5 | 4 workers chacun |
| Redis Cluster | 3 | 4GB RAM chacun |
| Ollama | 6 | NUM_PARALLEL=4 chacun |
| **Total GPU** | 2-3 | A100 ou 3x RTX 4090 |

**DÃ©bit estimÃ© :** 100-150 requÃªtes/minute (avec cache: 300+/min)

---

## RÃ©sumÃ© des Actions

### ImmÃ©diat (PC Local - 2-3 users)
1. âœ… Configurer `OLLAMA_NUM_PARALLEL=3`
2. âœ… Utiliser modÃ¨le quantifiÃ© `mistral:7b-instruct-q4_0`
3. âœ… RÃ©duire `num_predict` Ã  200
4. âœ… Lancer Uvicorn avec `--workers 2`

### Court Terme (Serveur - 50 users)
1. ðŸ”² ImplÃ©menter le cache sÃ©mantique Redis
2. ðŸ”² Lancer 2 instances Ollama
3. ðŸ”² Ajouter NGINX en frontal

### Moyen Terme (Serveur - 100+ users)
1. ðŸ”² Docker Compose multi-services
2. ðŸ”² OllamaPool avec load balancing
3. ðŸ”² GPU dÃ©diÃ© (RTX 4090 minimum)
4. ðŸ”² Monitoring (Prometheus + Grafana)

### Long Terme (200+ users)
1. ðŸ”² Kubernetes pour l'auto-scaling
2. ðŸ”² Migration vers vLLM (meilleur batching)
3. ðŸ”² CDN pour les assets frontend
4. ðŸ”² Multi-rÃ©gion si nÃ©cessaire

---

## Commandes Utiles

### DÃ©marrer en mode parallÃ¨le (Windows)
```powershell
# Terminal 1 - Ollama
$env:OLLAMA_NUM_PARALLEL = "3"
ollama serve

# Terminal 2 - Backend
cd backend
python -m uvicorn main:app --host 0.0.0.0 --port 8000 --workers 2
```

### Monitoring des performances
```powershell
# Voir les requÃªtes Ollama en cours
curl http://localhost:11434/api/ps

# Tester la charge
hey -n 100 -c 10 -m POST -H "Content-Type: application/json" -d '{"question":"Quels formats proposez-vous?"}' http://localhost:8000/api/v1/chat
```

### VÃ©rifier la santÃ© du systÃ¨me
```powershell
# MÃ©moire utilisÃ©e par Ollama
Get-Process ollama* | Select-Object Name, WorkingSet64
```
