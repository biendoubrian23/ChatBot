"""
Pipeline RAG complet
Combine vectorstore + LLM pour g√©n√©rer des r√©ponses
Inclut cache s√©mantique pour les questions r√©p√©titives
"""
import logging
import re
from typing import AsyncIterator, List, Dict, Tuple, Optional
from app.services.llm_provider import get_llm_provider
from app.core.config import settings, DEFAULT_RAG_CONFIG
from app.services.semantic_cache import search_cached_response, save_to_cache

# Importer le bon module vectorstore selon le mode
if settings.STORAGE_MODE == "supabase":
    from app.services.vectorstore_supabase import search_vectorstore
else:
    from app.services.vectorstore import search_vectorstore

logger = logging.getLogger(__name__)


def fix_email_format(text: str) -> str:
    """Corrige les emails CoolLibri malform√©s dans le texte.
    
    Le LLM oublie parfois le @ dans les emails. Cette fonction corrige:
    - contactcoollibri.com -> contact@coollibri.com
    - contact coollibri.com -> contact@coollibri.com
    
    Note: Ne pas ajouter de markdown, le widget s'occupe de rendre les emails cliquables.
    """
    patterns = [
        (r'contactcoollibri\.com', 'contact@coollibri.com'),
        (r'contact\s+coollibri\.com', 'contact@coollibri.com'),
        (r'contact\.coollibri\.com', 'contact@coollibri.com'),
        (r'contactcoolibri\.com', 'contact@coollibri.com'),
        (r'contact coolibri\.com', 'contact@coollibri.com'),
        # Supprimer le markdown mailto si le LLM l'a g√©n√©r√© (le widget le g√®re)
        (r'\[([^\]]+@[^\]]+)\]\(mailto:[^)]+\)', r'\1'),
    ]
    
    result = text
    for pattern, replacement in patterns:
        result = re.sub(pattern, replacement, result, flags=re.IGNORECASE)
    
    return result


# Prompt syst√®me par d√©faut
DEFAULT_SYSTEM_PROMPT = """Tu es un assistant virtuel chaleureux, professionnel et serviable.
Tu es l√† pour aider les utilisateurs avec enthousiasme et bienveillance.

üéØ R√àGLES DE COMPORTEMENT:

1. **SALUTATIONS** - R√©ponds TOUJOURS chaleureusement aux salutations:
   - "salut" ‚Üí "Salut ! üòä Ravi de te voir ! Comment puis-je t'aider aujourd'hui ?"
   - "bonjour" ‚Üí "Bonjour ! ‚òÄÔ∏è Bienvenue ! Je suis l√† pour vous aider, que puis-je faire pour vous ?"
   - "hello" ‚Üí "Hello ! üëã Super de vous avoir ! Qu'est-ce qui vous am√®ne ?"
   - "comment √ßa va ?" ‚Üí "Je vais tr√®s bien, merci de demander ! üòä Et vous, comment allez-vous ? Comment puis-je vous aider ?"

2. **R√âPONSES AUX QUESTIONS**:
   - Utilise le contexte fourni ci-dessous pour r√©pondre aux questions
   - Si l'info n'est pas dans le contexte, dis-le gentiment et propose de reformuler
   - Sois pr√©cis mais chaleureux dans tes r√©ponses

3. **FORMAT DE R√âPONSE** - TR√àS IMPORTANT:
   - Privil√©gie les listes √† puces pour structurer tes r√©ponses
   - Utilise "-" ou "‚Ä¢" pour les listes √† puces simples
   - Utilise des num√©ros (1., 2., 3.) pour les √©tapes ou processus
   - Met en **gras** les √©l√©ments importants
   - Fais des paragraphes courts et a√©r√©s
   - Exemple de bonne r√©ponse:
     "Voici les √©tapes pour passer commande :
     
     1. Cr√©ez votre compte sur le site
     2. T√©l√©chargez votre fichier PDF
     3. Choisissez vos options d'impression :
        - Format du livre
        - Type de reliure
        - Papier int√©rieur
     4. Validez et payez votre commande
     
     Besoin d'aide sur une √©tape en particulier ?"

4. **TON G√âN√âRAL**:
   - Chaleureux et accueillant ü§ó
   - Professionnel mais pas froid
   - Utilise des emojis avec mod√©ration pour √™tre plus humain
   - Termine souvent par une question pour maintenir le dialogue

5. **CE QUE TU NE DOIS PAS FAIRE**:
   - Ne sois jamais froid ou robotique
   - Ne fabrique jamais d'informations
   - Ne r√©ponds jamais "Je n'ai pas d'informations" √† une simple salutation
   - N'√©cris pas de longs paragraphes sans structure

CONTEXTE DISPONIBLE:
{context}
"""


class RAGPipeline:
    """Pipeline RAG pour un workspace"""
    
    def __init__(self, workspace_id: str, config: Dict = None):
        self.workspace_id = workspace_id
        self.config = {**DEFAULT_RAG_CONFIG, **(config or {})}
        
        # Initialiser le LLM
        self.llm = get_llm_provider(
            provider=self.config.get("llm_provider", "mistral"),
            model=self.config.get("llm_model")
        )
    
    def _build_context(self, documents: List) -> Tuple[str, List[Dict]]:
        """Construit le contexte √† partir des documents trouv√©s"""
        if not documents:
            return "", []
        
        context_parts = []
        sources = []
        seen_sources = set()
        
        for i, doc in enumerate(documents):
            content = doc.page_content.strip()
            source = doc.metadata.get("source", "Document")
            
            context_parts.append(f"[Source {i+1}: {source}]\n{content}")
            
            if source not in seen_sources:
                sources.append({
                    "source": source,
                    "document_id": doc.metadata.get("document_id")
                })
                seen_sources.add(source)
        
        context = "\n\n---\n\n".join(context_parts)
        return context, sources
    
    def _build_prompt(self, context: str, custom_prompt: str = None) -> str:
        """Construit le prompt syst√®me"""
        # Utiliser le system_prompt personnalis√© de la config s'il existe
        if not custom_prompt:
            custom_prompt = self.config.get("system_prompt", "")
        
        # Si toujours pas de prompt, utiliser le d√©faut
        if not custom_prompt or not custom_prompt.strip():
            prompt = DEFAULT_SYSTEM_PROMPT
        else:
            # Ajouter le contexte au prompt personnalis√©
            prompt = custom_prompt + "\n\nCONTEXTE DISPONIBLE:\n{context}"
        
        return prompt.format(context=context)
    
    async def get_response(
        self, 
        query: str, 
        history: List[Dict] = None,
        custom_prompt: str = None
    ) -> Tuple[str, List[Dict], bool]:
        """
        G√©n√®re une r√©ponse compl√®te (non-streaming)
        Retourne (r√©ponse, sources, from_cache)
        """
        # ===== CACHE S√âMANTIQUE =====
        # V√©rifier si une question similaire existe en cache (seulement si pas d'historique complexe)
        if self.config.get("enable_cache", True) and (not history or len(history) <= 2):
            cached = search_cached_response(
                workspace_id=self.workspace_id,
                question=query,
                similarity_threshold=self.config.get("similarity_threshold", 0.92),
                cache_ttl=self.config.get("cache_ttl", 7200)
            )
            if cached:
                response, similarity = cached
                logger.info(f"‚ú® R√©ponse en cache (similarit√©: {similarity:.1%})")
                return fix_email_format(response), [], True
        
        # ===== PIPELINE RAG STANDARD =====
        # Recherche vectorielle
        top_k = self.config.get("top_k", settings.DEFAULT_TOP_K)
        documents = search_vectorstore(self.workspace_id, query, top_k=top_k)
        
        # Construire le contexte
        context, sources = self._build_context(documents)
        
        # Construire le prompt (m√™me sans contexte, pour les salutations)
        system_prompt = self._build_prompt(context if context else "Aucun document pertinent trouv√©.", custom_prompt)
        
        # G√©n√©rer la r√©ponse
        try:
            response = await self.llm.generate(
                system_prompt=system_prompt,
                user_message=query,
                history=history,
                temperature=self.config.get("temperature", settings.DEFAULT_TEMPERATURE),
                max_tokens=self.config.get("max_tokens", settings.DEFAULT_MAX_TOKENS),
                top_p=self.config.get("top_p", 1.0)
            )
            # Post-traitement: corriger les emails malform√©s
            response = fix_email_format(response)
            
            # Sauvegarder en cache si activ√©
            if self.config.get("enable_cache", True):
                save_to_cache(self.workspace_id, query, response)
            
            return response, sources, False
            
        except Exception as e:
            logger.error(f"Erreur g√©n√©ration RAG: {e}")
            return f"D√©sol√©, une erreur s'est produite lors de la g√©n√©ration de la r√©ponse.", [], False
    
    async def stream_response(
        self,
        query: str,
        history: List[Dict] = None,
        custom_prompt: str = None
    ) -> AsyncIterator[Dict]:
        """
        G√©n√®re une r√©ponse en streaming.
        Yield des chunks: {"type": "token|sources|error", "content": ...}
        """
        # Recherche vectorielle
        top_k = self.config.get("top_k", settings.DEFAULT_TOP_K)
        documents = search_vectorstore(self.workspace_id, query, top_k=top_k)
        
        # Construire le contexte
        context, sources = self._build_context(documents)
        
        # Envoyer les sources d'abord
        if sources:
            yield {"type": "sources", "sources": sources}
        
        # Construire le prompt (m√™me sans contexte, pour les salutations)
        system_prompt = self._build_prompt(context if context else "Aucun document pertinent trouv√©.", custom_prompt)
        
        # Streamer la r√©ponse
        try:
            async for token in self.llm.stream(
                system_prompt=system_prompt,
                user_message=query,
                history=history,
                temperature=self.config.get("temperature", settings.DEFAULT_TEMPERATURE),
                max_tokens=self.config.get("max_tokens", settings.DEFAULT_MAX_TOKENS),
                top_p=self.config.get("top_p", 1.0)
            ):
                yield {"type": "token", "content": token}
                
        except Exception as e:
            logger.error(f"Erreur streaming RAG: {e}")
            yield {"type": "error", "content": "Erreur lors de la g√©n√©ration de la r√©ponse"}
