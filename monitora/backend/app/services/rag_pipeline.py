"""
Pipeline RAG complet
Combine vectorstore + LLM pour g√©n√©rer des r√©ponses
"""
import logging
from typing import AsyncIterator, List, Dict, Tuple, Optional
from app.services.llm_provider import get_llm_provider
from app.core.config import settings, DEFAULT_RAG_CONFIG

# Importer le bon module vectorstore selon le mode
if settings.STORAGE_MODE == "supabase":
    from app.services.vectorstore_supabase import search_vectorstore
else:
    from app.services.vectorstore import search_vectorstore

logger = logging.getLogger(__name__)

# Prompt syst√®me par d√©faut
DEFAULT_SYSTEM_PROMPT = """Tu es un assistant virtuel chaleureux, professionnel et serviable.
Tu es l√† pour aider les utilisateurs avec enthousiasme et bienveillance.

üéØ R√àGLES DE COMPORTEMENT:

1. **SALUTATIONS** - R√©ponds TOUJOURS chaleureusement aux salutations:
   - "salut" ‚Üí "Salut ! üòä Ravi de te voir ! Comment puis-je t'aider aujourd'hui ?"
   - "bonjour" ‚Üí "Bonjour ! ‚òÄÔ∏è Bienvenue ! Je suis l√† pour vous aider, que puis-je faire pour vous ?"
   - "hello" ‚Üí "Hello ! üëã Super de vous avoir ! Qu'est-ce qui vous am√®ne ?"
   - "comment √ßa va ?" ‚Üí "Je vais tr√®s bien, merci de demander ! üòä Et vous, comment allez-vous ? Comment puis-je vous aider ?"

2. **R√âPONSES AUX QUESTIONS**:
   - Utilise le contexte fourni ci-dessous pour r√©pondre aux questions
   - Si l'info n'est pas dans le contexte, dis-le gentiment et propose de reformuler
   - Sois pr√©cis mais chaleureux dans tes r√©ponses

3. **TON G√âN√âRAL**:
   - Chaleureux et accueillant ü§ó
   - Professionnel mais pas froid
   - Utilise des emojis avec mod√©ration pour √™tre plus humain
   - Termine souvent par une question pour maintenir le dialogue

4. **CE QUE TU NE DOIS PAS FAIRE**:
   - Ne sois jamais froid ou robotique
   - Ne fabrique jamais d'informations
   - Ne r√©ponds jamais "Je n'ai pas d'informations" √† une simple salutation

CONTEXTE DISPONIBLE:
{context}
"""


class RAGPipeline:
    """Pipeline RAG pour un workspace"""
    
    def __init__(self, workspace_id: str, config: Dict = None):
        self.workspace_id = workspace_id
        self.config = {**DEFAULT_RAG_CONFIG, **(config or {})}
        
        # Initialiser le LLM
        self.llm = get_llm_provider(
            provider=self.config.get("llm_provider", "mistral"),
            model=self.config.get("llm_model")
        )
    
    def _build_context(self, documents: List) -> Tuple[str, List[Dict]]:
        """Construit le contexte √† partir des documents trouv√©s"""
        if not documents:
            return "", []
        
        context_parts = []
        sources = []
        seen_sources = set()
        
        for i, doc in enumerate(documents):
            content = doc.page_content.strip()
            source = doc.metadata.get("source", "Document")
            
            context_parts.append(f"[Source {i+1}: {source}]\n{content}")
            
            if source not in seen_sources:
                sources.append({
                    "source": source,
                    "document_id": doc.metadata.get("document_id")
                })
                seen_sources.add(source)
        
        context = "\n\n---\n\n".join(context_parts)
        return context, sources
    
    def _build_prompt(self, context: str, custom_prompt: str = None) -> str:
        """Construit le prompt syst√®me"""
        # Utiliser le system_prompt personnalis√© de la config s'il existe
        if not custom_prompt:
            custom_prompt = self.config.get("system_prompt", "")
        
        # Si toujours pas de prompt, utiliser le d√©faut
        if not custom_prompt or not custom_prompt.strip():
            prompt = DEFAULT_SYSTEM_PROMPT
        else:
            # Ajouter le contexte au prompt personnalis√©
            prompt = custom_prompt + "\n\nCONTEXTE DISPONIBLE:\n{context}"
        
        return prompt.format(context=context)
    
    async def get_response(
        self, 
        query: str, 
        history: List[Dict] = None,
        custom_prompt: str = None
    ) -> Tuple[str, List[Dict]]:
        """
        G√©n√®re une r√©ponse compl√®te (non-streaming)
        Retourne (r√©ponse, sources)
        """
        # Recherche vectorielle
        top_k = self.config.get("top_k", settings.DEFAULT_TOP_K)
        documents = search_vectorstore(self.workspace_id, query, top_k=top_k)
        
        # Construire le contexte
        context, sources = self._build_context(documents)
        
        # Construire le prompt (m√™me sans contexte, pour les salutations)
        system_prompt = self._build_prompt(context if context else "Aucun document pertinent trouv√©.", custom_prompt)
        
        # G√©n√©rer la r√©ponse
        try:
            response = await self.llm.generate(
                system_prompt=system_prompt,
                user_message=query,
                history=history,
                temperature=self.config.get("temperature", settings.DEFAULT_TEMPERATURE),
                max_tokens=self.config.get("max_tokens", settings.DEFAULT_MAX_TOKENS),
                top_p=self.config.get("top_p", 1.0)
            )
            return response, sources
            
        except Exception as e:
            logger.error(f"Erreur g√©n√©ration RAG: {e}")
            return f"D√©sol√©, une erreur s'est produite lors de la g√©n√©ration de la r√©ponse.", []
    
    async def stream_response(
        self,
        query: str,
        history: List[Dict] = None,
        custom_prompt: str = None
    ) -> AsyncIterator[Dict]:
        """
        G√©n√®re une r√©ponse en streaming.
        Yield des chunks: {"type": "token|sources|error", "content": ...}
        """
        # Recherche vectorielle
        top_k = self.config.get("top_k", settings.DEFAULT_TOP_K)
        documents = search_vectorstore(self.workspace_id, query, top_k=top_k)
        
        # Construire le contexte
        context, sources = self._build_context(documents)
        
        # Envoyer les sources d'abord
        if sources:
            yield {"type": "sources", "sources": sources}
        
        # Construire le prompt (m√™me sans contexte, pour les salutations)
        system_prompt = self._build_prompt(context if context else "Aucun document pertinent trouv√©.", custom_prompt)
        
        # Streamer la r√©ponse
        try:
            async for token in self.llm.stream(
                system_prompt=system_prompt,
                user_message=query,
                history=history,
                temperature=self.config.get("temperature", settings.DEFAULT_TEMPERATURE),
                max_tokens=self.config.get("max_tokens", settings.DEFAULT_MAX_TOKENS),
                top_p=self.config.get("top_p", 1.0)
            ):
                yield {"type": "token", "content": token}
                
        except Exception as e:
            logger.error(f"Erreur streaming RAG: {e}")
            yield {"type": "error", "content": "Erreur lors de la g√©n√©ration de la r√©ponse"}
