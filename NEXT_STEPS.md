# üöÄ Prochaines √©tapes - LibriAssist

Ce document vous guide pour d√©marrer et tester LibriAssist.

---

## üìã Checklist avant de commencer

### ‚úÖ Pr√©requis √† installer

- [ ] **Python 3.9+** install√© ‚Üí [T√©l√©charger](https://www.python.org/downloads/)
- [ ] **Node.js 18+** install√© ‚Üí [T√©l√©charger](https://nodejs.org/)
- [ ] **Ollama** install√© ‚Üí [T√©l√©charger](https://ollama.ai/)
- [ ] **Git** install√© (optionnel) ‚Üí [T√©l√©charger](https://git-scm.com/)

### ‚úÖ V√©rification rapide

Ouvrez PowerShell et testez :

```powershell
python --version    # Doit afficher Python 3.9+
node --version      # Doit afficher v18+
npm --version       # Doit afficher npm
ollama --version    # Doit afficher ollama version
```

---

## üéØ √âtape 1 : Installation (10 minutes)

### Option A : Installation automatique (recommand√©)

```powershell
# Dans le dossier CHATBOT
.\install.ps1
```

Ce script va :
1. ‚úÖ V√©rifier Python et Node.js
2. ‚úÖ Cr√©er l'environnement virtuel Python
3. ‚úÖ Installer toutes les d√©pendances backend
4. ‚úÖ Installer toutes les d√©pendances frontend
5. ‚úÖ V√©rifier Ollama et proposer de t√©l√©charger Mistral

### Option B : Installation manuelle

Si le script ne fonctionne pas :

```powershell
# Backend
cd backend
python -m venv venv
.\venv\Scripts\Activate.ps1
pip install -r requirements.txt
cd ..

# Frontend
cd frontend
npm install
cd ..
```

### T√©l√©charger le mod√®le Mistral

```powershell
ollama pull mistral:7b
```

‚è±Ô∏è **Temps estim√©** : 5-10 minutes (selon connexion internet)

---

## üìö √âtape 2 : Indexer les documents (2 minutes)

```powershell
cd backend
.\venv\Scripts\Activate.ps1
python scripts\index_documents.py
```

**Ce que fait cette commande** :
1. üìÑ Lit le fichier `FAQ CoolLibri.pdf`
2. ‚úÇÔ∏è D√©coupe le texte en chunks intelligents
3. üß† G√©n√®re les embeddings avec SentenceTransformers
4. üíæ Stocke tout dans ChromaDB

**R√©sultat attendu** :
```
üìö LibriAssist - Document Indexer
Processing FAQ CoolLibri.pdf
  ‚Üí Created XX chunks
‚úÖ Indexing complete!
üìä Total documents in vector store: XX
```

Si vous voyez √ßa, c'est parfait ! ‚úÖ

---

## üöÄ √âtape 3 : D√©marrer le syst√®me

### Option A : D√©marrage automatique (recommand√©)

```powershell
# Dans le dossier CHATBOT
.\start.ps1
```

Ce script va :
1. ‚úÖ V√©rifier qu'Ollama tourne
2. ‚úÖ Ouvrir une fen√™tre pour le backend
3. ‚úÖ Ouvrir une fen√™tre pour le frontend
4. ‚úÖ Ouvrir votre navigateur sur http://localhost:3000

### Option B : D√©marrage manuel

**Terminal 1 - Backend** :
```powershell
cd backend
.\venv\Scripts\Activate.ps1
python main.py
```

Attendez de voir :
```
üöÄ Starting LibriAssist API...
‚úÖ LibriAssist API is ready!
üìç Listening on http://0.0.0.0:8000
```

**Terminal 2 - Frontend** :
```powershell
cd frontend
npm run dev
```

Attendez de voir :
```
‚ñ≤ Next.js 14.1.0
- Local: http://localhost:3000
‚úì Ready in X.Xs
```

---

## üß™ √âtape 4 : Tester le chatbot

1. **Ouvrir le navigateur** : http://localhost:3000

2. **Vous devriez voir** :
   - Logo LibriAssist (LA) en gradient bleu-violet
   - Message de bienvenue
   - 4 suggestions de questions
   - Zone de saisie en bas

3. **Tester une question** :
   - Cliquez sur une suggestion OU
   - Tapez : "Comment fonctionne CoolLibri ?"
   - Appuyez sur Entr√©e ou cliquez sur le bouton d'envoi

4. **R√©sultat attendu** :
   - ‚è≥ Indicateur de chargement (3 points qui bougent)
   - üí¨ R√©ponse du chatbot apr√®s quelques secondes
   - üìö Sources affich√©es sous la r√©ponse avec scores de pertinence
   - ‚è∞ Timestamp de la r√©ponse

---

## ‚úÖ Tests recommand√©s

### Test 1 : Questions basiques
```
‚ùì "Qu'est-ce que CoolLibri ?"
‚ùì "Comment cr√©er un compte ?"
‚ùì "Quels sont les tarifs ?"
```

### Test 2 : Questions sp√©cifiques
```
‚ùì "Comment r√©silier mon abonnement ?"
‚ùì "Quelles sont les modalit√©s de paiement ?"
‚ùì "Comment contacter le support ?"
```

### Test 3 : Nouvelle conversation
- Cliquez sur "Nouvelle conversation" en haut √† droite
- L'historique doit se r√©initialiser
- L'√©cran de bienvenue doit r√©appara√Ætre

### Test 4 : Mode sombre
- Changez le th√®me de votre syst√®me (Windows : Param√®tres ‚Üí Personnalisation)
- L'interface doit s'adapter automatiquement

---

## üîç V√©rifier que tout fonctionne

### Backend (API)

**Health check** : http://localhost:8000/api/v1/health

R√©sultat attendu :
```json
{
  "status": "healthy",
  "version": "1.0.0",
  "ollama_available": true,
  "vectorstore_loaded": true
}
```

**Documentation API** : http://localhost:8000/docs
- Vous devez voir l'interface Swagger
- 3 endpoints visibles : /chat, /health, /stats

**Stats** : http://localhost:8000/api/v1/stats

R√©sultat attendu :
```json
{
  "total_documents": XX,
  "collection_name": "coolibri_docs"
}
```

### Frontend

**Console d√©veloppeur** (F12) :
- Aucune erreur en rouge
- Peut avoir des warnings (normal)

**Network tab** :
- Requ√™tes vers `http://localhost:8000/api/v1/chat` avec status 200

---

## ‚ùå D√©pannage rapide

### "Ollama not available"

```powershell
# Ouvrir un nouveau terminal
ollama serve
```

### "ECONNREFUSED localhost:8000"

Le backend n'est pas d√©marr√©. Lancez :
```powershell
cd backend
.\venv\Scripts\Activate.ps1
python main.py
```

### R√©ponse tr√®s lente (>30 secondes)

- Normal la premi√®re fois (chargement du mod√®le)
- Ensuite devrait √™tre ~3-10 secondes

### Erreur "Module not found"

**Backend** :
```powershell
cd backend
.\venv\Scripts\Activate.ps1
pip install -r requirements.txt --force-reinstall
```

**Frontend** :
```powershell
cd frontend
rm -rf node_modules
npm install
```

### Documents non trouv√©s

```powershell
# V√©rifier que le PDF est bien l√†
dir docs\*.pdf

# R√©indexer
cd backend
.\venv\Scripts\Activate.ps1
python scripts\index_documents.py
```

---

## üìä Performances attendues

| M√©trique | Valeur attendue |
|----------|----------------|
| Temps de r√©ponse | 3-10 secondes |
| Pr√©cision | √âlev√©e si info dans FAQ |
| Sources affich√©es | 1-3 documents |
| Score de pertinence | >0.7 pour bonnes r√©ponses |
| Utilisation CPU | Mod√©r√©e (LLM sur CPU) |
| Utilisation RAM | ~2-4 GB |

---

## üéØ Prochaines actions

Une fois que tout fonctionne :

1. **Ajouter vos propres PDF** :
   - Placer les PDF dans `docs/`
   - R√©ex√©cuter `python scripts\index_documents.py`

2. **Personnaliser le prompt** :
   - √âditer `backend/app/services/llm.py`
   - Modifier la variable `system_prompt`

3. **Ajuster les param√®tres RAG** :
   - √âditer `backend/.env`
   - Modifier `CHUNK_SIZE`, `TOP_K_RESULTS`, etc.

4. **Customiser le design** :
   - √âditer les composants dans `frontend/components/`
   - Modifier `tailwind.config.js` pour les couleurs

5. **D√©ployer en production** :
   - Consulter la section d√©ploiement du README.md
   - Configurer Nginx + Gunicorn + PM2

---

## üìö Documentation

- **README.md** : Documentation compl√®te
- **QUICKSTART.md** : Guide de d√©marrage rapide
- **PROJECT_SUMMARY.md** : R√©sum√© technique du projet
- **Backend** : http://localhost:8000/docs (Swagger)

---

## üí¨ Besoin d'aide ?

1. Consultez le [README.md](README.md)
2. V√©rifiez les logs dans les terminaux
3. Testez le health check : http://localhost:8000/api/v1/health
4. Consultez la console navigateur (F12)

---

## ‚úÖ Checklist finale

Avant de dire "√ßa marche !" :

- [ ] Installation compl√®te (backend + frontend)
- [ ] Ollama install√© et Mistral t√©l√©charg√©
- [ ] Documents index√©s avec succ√®s
- [ ] Backend d√©marre sans erreur
- [ ] Frontend d√©marre sans erreur
- [ ] Interface accessible sur localhost:3000
- [ ] Question test fonctionne
- [ ] R√©ponse g√©n√©r√©e avec sources
- [ ] Health check retourne "healthy"

Si tous les points sont ‚úÖ, **f√©licitations** ! üéâ

**LibriAssist est op√©rationnel !** üöÄ

---

Bon d√©veloppement ! üíª‚ú®
